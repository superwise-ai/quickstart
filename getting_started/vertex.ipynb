{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superwise-ai/quickstart/blob/main/getting_started/vertex.ipynb#offline=true&sandboxMode=true\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title",
        "tags": []
      },
      "source": [
        "# 🚀 Getting started with Superwise.ai on GCP Vertex AI\n",
        "\n",
        "In this notebook, we will demonstrate how to integrate a Vertex AI based development workflow with Superwise.ai\n",
        "\n",
        "**Part I** of this notebook walks you through building a classical model for predicting the Titanic passenger survival, using Sci-kit learn on Vertex AI. \n",
        "\n",
        "It is based on [GCP tutorial for building custom models on Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb).\n",
        "\n",
        "**Part II** of this notebook will walk you through how to setup Superwise.ai to start tracking your model, by registering and providing a baseline for the model's behavior.\n",
        "\n",
        "**Part III** will demonstrate how to send new predictions from your model to Superwise.ai, simulating a post-deployment scenario.\n",
        "\n",
        "At this point, you should be able to start seeing insights from Superwise.ai in the web portal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ig8dAeyImQeV"
      },
      "source": [
        "## 📌 Prerequisites\n",
        "\n",
        "1. A Superwise.ai account that enables you to login and view insights + Superwise SDK installed\n",
        "2. A set of API keys for sending data to Superwise.ai \n",
        "3. Permissions to create models, training jobs and inference endpoints inside Vertex.ai\n",
        "4. Grant Superwise.ai permissions to your GCS bucket #soon to be removed\n",
        "\n",
        "Note: this notebook works best when run from within a Vertex AI notebook instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfkHAOzNmQeW"
      },
      "outputs": [],
      "source": [
        "%pip install -U superwise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:custom"
      },
      "source": [
        "## 🏗️ Part I - building a Vertex AI Model to predict the survival chances of the Titanic passengers\n",
        "\n",
        "This is a classical SVM model, over a publicly available dataset.\n",
        "\n",
        "This guide is based on the best practices from [Vertex AI's example for building a Scikit-Learn model.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "### 🔧 Setup\n",
        "\n",
        "Install the latest version of Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fd00fa70a2a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsxCgt1zlugo"
      },
      "outputs": [],
      "source": [
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_storage"
      },
      "source": [
        "Install the latest version of *google-cloud-storage* library as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qssss-KSlugo"
      },
      "outputs": [],
      "source": [
        "! pip install {USER_FLAG} -U google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3hYaR6gLEK-"
      },
      "source": [
        "Install the latest version of *google-cloud-bigquery* library as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed everything, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzPxhxS5lugp",
        "outputId": "75e0bed4-9ae5-4b6a-9259-d1548c125bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you might be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "PROJECT_ID = shell_output[0]\n",
        "print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_project_id"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USd_pUT0lugr"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### 🛂 Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF60K5v1lugs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "metadata": {
        "id": "BXHzPdWTmL_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = f\"{PROJECT_ID}-superwise-vertex-demo-bucket\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"-aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz8J0vmSlugt"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oadE10x2lugu"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_aip",
        "tags": []
      },
      "source": [
        "### ➕ Import Vertex SDK for Python\n",
        "\n",
        "Import the Vertex SDK for Python into your Python environment and initialize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNEiwLd0lugu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "## Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "### 🔧 Set pre-built containers\n",
        "\n",
        "Vertex AI provides pre-built containers to run training and prediction.\n",
        "\n",
        "For the latest list, see [Pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) and [Pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1mr18jlugv",
        "outputId": "7d0bee53-3ef7-475d-e783-238a76b68eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n",
            "Deployment: us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\n"
          ]
        }
      ],
      "source": [
        "TRAIN_VERSION = \"scikit-learn-cpu.0-23\"\n",
        "DEPLOY_VERSION = \"sklearn-cpu.0-23\"\n",
        "\n",
        "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "### Set machine types\n",
        "\n",
        "Next, set the machine types to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure your compute resources for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*.\n",
        "\n",
        "Learn [which machine types are available for training](https://cloud.google.com/vertex-ai/docs/training/configure-compute) and [which machine types are available for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAXwbqKKlugv",
        "outputId": "4c80c541-2f25-4d5d-df78-3be2d9a26e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train machine type n1-standard-4\n",
            "Deploy machine type n1-standard-2\n"
          ]
        }
      ],
      "source": [
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"2\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f24e7d2269"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZADK3gAmQei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJPMSaT_mQei"
      },
      "outputs": [],
      "source": [
        "BINARY_FEATURES = [\n",
        "    'sex']\n",
        "\n",
        "# List all column names for numeric features\n",
        "NUMERIC_FEATURES = [\n",
        "    'age',\n",
        "    'fare']\n",
        "\n",
        "# List all column names for categorical features\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'pclass',\n",
        "    'embarked',\n",
        "    'home_dest',\n",
        "    'parch',\n",
        "    'sibsp']\n",
        "\n",
        "LABEL = ['survived']\n",
        "\n",
        "ALL_COLUMNS = BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES+LABEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "890d562c6291"
      },
      "outputs": [],
      "source": [
        "# download the dataset\n",
        "df = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')\n",
        "df = df.rename(columns={\"home.dest\" : \"home_dest\"})\n",
        "df = df[ALL_COLUMNS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMM7BS71mQei",
        "outputId": "e5a22647-4618-489f-cc9d-5a70bddd8c29"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass</th>\n",
              "      <th>embarked</th>\n",
              "      <th>home_dest</th>\n",
              "      <th>parch</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>female</td>\n",
              "      <td>29</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "      <td>St Louis, MO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>151.55</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>female</td>\n",
              "      <td>2</td>\n",
              "      <td>151.55</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>30</td>\n",
              "      <td>151.55</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>25</td>\n",
              "      <td>151.55</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1304</th>\n",
              "      <td>female</td>\n",
              "      <td>14.5</td>\n",
              "      <td>14.4542</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>female</td>\n",
              "      <td>?</td>\n",
              "      <td>14.4542</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>male</td>\n",
              "      <td>26.5</td>\n",
              "      <td>7.225</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>male</td>\n",
              "      <td>27</td>\n",
              "      <td>7.225</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>male</td>\n",
              "      <td>29</td>\n",
              "      <td>7.875</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1309 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sex     age      fare  pclass embarked  \\\n",
              "0     female      29  211.3375       1        S   \n",
              "1       male  0.9167    151.55       1        S   \n",
              "2     female       2    151.55       1        S   \n",
              "3       male      30    151.55       1        S   \n",
              "4     female      25    151.55       1        S   \n",
              "...      ...     ...       ...     ...      ...   \n",
              "1304  female    14.5   14.4542       3        C   \n",
              "1305  female       ?   14.4542       3        C   \n",
              "1306    male    26.5     7.225       3        C   \n",
              "1307    male      27     7.225       3        C   \n",
              "1308    male      29     7.875       3        S   \n",
              "\n",
              "                            home_dest  parch  sibsp  survived  \n",
              "0                        St Louis, MO      0      0         1  \n",
              "1     Montreal, PQ / Chesterville, ON      2      1         1  \n",
              "2     Montreal, PQ / Chesterville, ON      2      1         0  \n",
              "3     Montreal, PQ / Chesterville, ON      2      1         0  \n",
              "4     Montreal, PQ / Chesterville, ON      2      1         0  \n",
              "...                               ...    ...    ...       ...  \n",
              "1304                                ?      0      1         0  \n",
              "1305                                ?      0      1         0  \n",
              "1306                                ?      0      0         0  \n",
              "1307                                ?      0      0         0  \n",
              "1308                                ?      0      0         0  \n",
              "\n",
              "[1309 rows x 9 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLJajyJFmQei"
      },
      "outputs": [],
      "source": [
        "def clean_missing_numerics(df: pd.DataFrame, numeric_columns):\n",
        "    '''\n",
        "    removes invalid values in the numeric columns\n",
        "\n",
        "            Parameters:\n",
        "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
        "                    numeric_columns (List[str]): List of column names that are numberic from the DataFrame\n",
        "            Returns:\n",
        "                    pandas.DataFrame: a dataframe with the numeric columns fixed\n",
        "    '''\n",
        "\n",
        "    for n in numeric_columns:\n",
        "        df[n] = pd.to_numeric(df[n], errors='coerce')\n",
        "\n",
        "    df = df.fillna(df.mean())\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmuN_C2OmQei"
      },
      "outputs": [],
      "source": [
        "df = clean_missing_numerics(df, NUMERIC_FEATURES)\n",
        "# add a record_id column, using the dataframe's natural index. This is needed for training so that later we can send the ID as part of the prediction payload\n",
        "df = df.reset_index().rename(columns = {'index': 'record_id'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZOCLtpHmQej"
      },
      "source": [
        "### Train/Test split and store as CSV files in the bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpMVJObrmQej"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = df.drop(columns=\"survived\")\n",
        "y = df[\"survived\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "train = X_train.copy()\n",
        "train[\"survived\"] = y_train\n",
        "\n",
        "test = X_test.copy()\n",
        "test[\"survived\"] = y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONmk9ys0mQej"
      },
      "outputs": [],
      "source": [
        "train.to_csv(f\"gs://{BUCKET_NAME}/data/titanic_train.csv\")\n",
        "test.to_csv(f\"gs://{BUCKET_NAME}/data/titanic_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7yzfC8mQej"
      },
      "source": [
        "## Prepare the training code package\n",
        "\n",
        "For this tutorial, we will wrap our training script in a package.\n",
        "This package can be run locally or installed inside the training container when running on the Vertex AI training machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMZ9UXpdmQej"
      },
      "outputs": [],
      "source": [
        "! mkdir -p titanic/trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPVIibODmQej",
        "outputId": "3aed7def-90b2-4092-e474-d02487d75a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting titanic/setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile titanic/setup.py\n",
        "\n",
        "from setuptools import find_packages\n",
        "from setuptools import setup\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'gcsfs==0.7.1',\n",
        "    'dask[dataframe]==2021.2.0',\n",
        "    'google-cloud-bigquery-storage==1.0.0',\n",
        "    'six==1.15.0'\n",
        "]\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=find_packages(), # Automatically find packages within this directory or below.\n",
        "    include_package_data=True, # if packages include any data files, those will be packed together.\n",
        "    description='Classification training titanic survivors prediction model'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM9kf4HemQek"
      },
      "outputs": [],
      "source": [
        "! touch titanic/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwpxuXM5mQek",
        "outputId": "77c5f77a-5218-4755-9c96-afd38f606860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting titanic/trainer/task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile titanic/trainer/task.py\n",
        "\n",
        "from google.cloud import bigquery, bigquery_storage, storage\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from typing import Union, List\n",
        "import os, logging, json, pickle, argparse\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# feature selection.  The FEATURE list defines what features are needed from the training data.\n",
        "# as well as the types of those features. We will perform different feature engineering depending on the type\n",
        "\n",
        "# List all column names for binary features: 0,1 or True,False or Male,Female etc\n",
        "BINARY_FEATURES = [\n",
        "    'sex']\n",
        "\n",
        "# List all column names for numeric features\n",
        "NUMERIC_FEATURES = [\n",
        "    'age',\n",
        "    'fare']\n",
        "\n",
        "# List all column names for categorical features\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'pclass',\n",
        "    'embarked',\n",
        "    'home_dest',\n",
        "    'parch',\n",
        "    'sibsp']\n",
        "\n",
        "# ID column - needed to support predict() over numpy arrays \n",
        "ID = ['record_id']\n",
        "\n",
        "ALL_COLUMNS = ID + BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES \n",
        "\n",
        "# define the column name for label\n",
        "LABEL = 'survived'\n",
        "\n",
        "\n",
        "# Define the index position of each feature. This is needed for processing a\n",
        "# numpy array (instead of pandas) which has no column names.\n",
        "BINARY_FEATURES_IDX = list(range(1,len(BINARY_FEATURES)+1))\n",
        "NUMERIC_FEATURES_IDX = list(range(len(BINARY_FEATURES)+1, len(BINARY_FEATURES)+len(NUMERIC_FEATURES)+1))\n",
        "CATEGORICAL_FEATURES_IDX = list(range(len(BINARY_FEATURES+NUMERIC_FEATURES)+1, len(ALL_COLUMNS)))\n",
        "\n",
        "\n",
        "def load_data_from_gcs(data_gcs_path: str) -> pd.DataFrame:\n",
        "    '''\n",
        "    Loads data from Google Cloud Storage (GCS) to a dataframe\n",
        "\n",
        "            Parameters:\n",
        "                    data_gcs_path (str): gs path for the location of the data. Wildcards are also supported. i.e gs://example_bucket/data/training-*.csv\n",
        "\n",
        "            Returns:\n",
        "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
        "    '''\n",
        "\n",
        "    # using dask that supports wildcards to read multiple files. Then with dd.read_csv().compute we create a pandas dataframe\n",
        "    # Additionally I have noticed that some values for TotalCharges are missing and this creates confusion regarding TotalCharges the data types.\n",
        "    # to overcome this we manually define TotalCharges as object.\n",
        "    # We will later fix this upnormality\n",
        "    logging.info(\"reading gs data: {}\".format(data_gcs_path))\n",
        "    return dd.read_csv(data_gcs_path, dtype={'TotalCharges': 'object'}).compute()\n",
        "\n",
        "\n",
        "def load_data_from_bq(bq_uri: str) -> pd.DataFrame:\n",
        "    '''\n",
        "    Loads data from BigQuery table (BQ) to a dataframe\n",
        "\n",
        "            Parameters:\n",
        "                    bq_uri (str): bq table uri. i.e: example_project.example_dataset.example_table\n",
        "            Returns:\n",
        "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
        "    '''\n",
        "    if not bq_uri.startswith('bq://'):\n",
        "        raise Exception(\"uri is not a BQ uri. It should be bq://project_id.dataset.table\")\n",
        "    logging.info(\"reading bq data: {}\".format(bq_uri))\n",
        "    project,dataset,table =  bq_uri.split(\".\")\n",
        "    bqclient = bigquery.Client(project=project[5:])\n",
        "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
        "    query_string = \"\"\"\n",
        "    SELECT * from {ds}.{tbl}\n",
        "    \"\"\".format(ds=dataset, tbl=table)\n",
        "\n",
        "    return (\n",
        "        bqclient.query(query_string)\n",
        "            .result()\n",
        "            .to_dataframe(bqstorage_client=bqstorageclient)\n",
        "    )\n",
        "\n",
        "def clean_missing_numerics(df: pd.DataFrame, numeric_columns):\n",
        "    '''\n",
        "    removes invalid values in the numeric columns\n",
        "\n",
        "            Parameters:\n",
        "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
        "                    numeric_columns (List[str]): List of column names that are numberic from the DataFrame\n",
        "            Returns:\n",
        "                    pandas.DataFrame: a dataframe with the numeric columns fixed\n",
        "    '''\n",
        "\n",
        "    for n in numeric_columns:\n",
        "        df[n] = pd.to_numeric(df[n], errors='coerce')\n",
        "\n",
        "    df = df.fillna(df.mean())\n",
        "\n",
        "    return df\n",
        "\n",
        "def data_selection(df: pd.DataFrame, selected_columns: List[str], label_column: str) -> (pd.DataFrame, pd.Series):\n",
        "    '''\n",
        "    From a dataframe it creates a new dataframe with only selected columns and returns it.\n",
        "    Additionally it splits the label column into a pandas Series.\n",
        "\n",
        "            Parameters:\n",
        "                    df (pandas.DataFrame): The Pandas Dataframe to drop columns and extract label\n",
        "                    selected_columns (List[str]): List of strings with the selected columns. i,e ['col_1', 'col_2', ..., 'col_n' ]\n",
        "                    label_column (str): The name of the label column\n",
        "\n",
        "            Returns:\n",
        "                    tuple(pandas.DataFrame, pandas.Series): Tuble with the new pandas DataFrame containing only selected columns and lablel pandas Series\n",
        "    '''\n",
        "    # We create a series with the prediciton label\n",
        "    labels = df[label_column]\n",
        "\n",
        "    data = df.loc[:, selected_columns]\n",
        "\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "def pipeline_builder(params_svm: dict, bin_ftr_idx: List[int], num_ftr_idx: List[int], cat_ftr_idx: List[int]) -> Pipeline:\n",
        "    '''\n",
        "    Builds a sklearn pipeline with preprocessing and model configuration.\n",
        "    Preprocessing steps are:\n",
        "        * OrdinalEncoder - used for binary features\n",
        "        * StandardScaler - used for numerical features\n",
        "        * OneHotEncoder - used for categorical features\n",
        "    Model used is SVC\n",
        "\n",
        "            Parameters:\n",
        "                    params_svm (dict): List of parameters for the sklearn.svm.SVC classifier\n",
        "                    bin_ftr_idx (List[str]): List of ints that mark the column indexes with binary columns. i.e [0, 2, ... , X ]\n",
        "                    num_ftr_idx (List[str]): List of ints that mark the column indexes with numerica columns. i.e [6, 3, ... , X ]\n",
        "                    cat_ftr_idx (List[str]): List of ints that mark the column indexes with categorical columns. i.e [5, 10, ... , X ]\n",
        "                    label_column (str): The name of the label column\n",
        "\n",
        "            Returns:\n",
        "                     Pipeline: sklearn.pipelines.Pipeline with preprocessing and model training\n",
        "    '''\n",
        "\n",
        "    # Definining a preprocessing step for our pipeline.\n",
        "    # it specifies how the features are going to be transformed\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('bin', OrdinalEncoder(), bin_ftr_idx),\n",
        "            ('num', StandardScaler(), num_ftr_idx),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'),  cat_ftr_idx)], remainder='drop', n_jobs=-1)\n",
        "\n",
        "\n",
        "    # We now create a full pipeline, for preprocessing and training.\n",
        "    # for training we selected a linear SVM classifier\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.set_params(**params_svm)\n",
        "\n",
        "    return Pipeline(steps=[ ('preprocessor', preprocessor),\n",
        "                            ('classifier', clf)])\n",
        "\n",
        "def train_pipeline(clf: Pipeline, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]) -> float:\n",
        "    '''\n",
        "    Trains a sklearn pipeline by fiting training data an labels and returns the accuracy f1 score\n",
        "\n",
        "            Parameters:\n",
        "                    clf (sklearn.pipelines.Pipeline): the Pipeline object to fit the data\n",
        "                    X: (pd.DataFrame OR np.ndarray): Training vectors of shape n_samples x n_features, where n_samples is the number of samples and n_features is the number of features.\n",
        "                    y: (pd.DataFrame OR np.ndarray): Labels of shape n_samples. Order should mathc Training Vectors X\n",
        "\n",
        "            Returns:\n",
        "                    score (float): Average F1 score from all cross validations\n",
        "    '''\n",
        "    # run cross validation to get training score. we can use this score to optimise training\n",
        "    score = cross_val_score(clf, X, y, cv=10, n_jobs=-1).mean()\n",
        "\n",
        "    # Now we fit all our data to the classifier.\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    return score\n",
        "\n",
        "def process_gcs_uri(uri: str) -> (str, str, str, str):\n",
        "    '''\n",
        "    Receives a Google Cloud Storage (GCS) uri and breaks it down to the scheme, bucket, path and file\n",
        "\n",
        "            Parameters:\n",
        "                    uri (str): GCS uri\n",
        "\n",
        "            Returns:\n",
        "                    scheme (str): uri scheme\n",
        "                    bucket (str): uri bucket\n",
        "                    path (str): uri path\n",
        "                    file (str): uri file\n",
        "    '''\n",
        "    url_arr = uri.split(\"/\")\n",
        "    if \".\" not in url_arr[-1]:\n",
        "        file = \"\"\n",
        "    else:\n",
        "        file = url_arr.pop()\n",
        "    scheme = url_arr[0]\n",
        "    bucket = url_arr[2]\n",
        "    path = \"/\".join(url_arr[3:])\n",
        "    path = path[:-1] if path.endswith(\"/\") else path\n",
        "\n",
        "    return scheme, bucket, path, file\n",
        "\n",
        "def pipeline_export_gcs(fitted_pipeline: Pipeline, model_dir: str) -> str:\n",
        "    '''\n",
        "    Exports trained pipeline to GCS\n",
        "\n",
        "            Parameters:\n",
        "                    fitted_pipeline (sklearn.pipelines.Pipeline): the Pipeline object with data already fitted (trained pipeline object)\n",
        "                    model_dir (str): GCS path to store the trained pipeline. i.e gs://example_bucket/training-job\n",
        "            Returns:\n",
        "                    export_path (str): Model GCS location\n",
        "    '''\n",
        "    scheme, bucket, path, file = process_gcs_uri(model_dir)\n",
        "    if scheme != \"gs:\":\n",
        "        raise ValueError(\"URI scheme must be gs\")\n",
        "\n",
        "    # Upload the model to GCS\n",
        "    b = storage.Client().bucket(bucket)\n",
        "    export_path = os.path.join(path, 'model.pkl')\n",
        "    blob = b.blob(export_path)\n",
        "\n",
        "    blob.upload_from_string(pickle.dumps(fitted_pipeline))\n",
        "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
        "\n",
        "\n",
        "def prepare_report(cv_score: float, model_params: dict, classification_report: str, columns: List[str], example_data: np.ndarray) -> str:\n",
        "    '''\n",
        "    Prepares a training report in Text\n",
        "\n",
        "            Parameters:\n",
        "                    cv_score (float): score of the training job during cross validation of training data\n",
        "                    model_params (dict): dictonary containing the parameters the model was trained with\n",
        "                    classification_report (str): Model classification report with test data\n",
        "                    columns (List[str]): List of columns that where used in training.\n",
        "                    example_data (np.array): Sample of data (2-3 rows are enough). This is used to include what the prediciton payload should look like for the model\n",
        "            Returns:\n",
        "                    report (str): Full report in text\n",
        "    '''\n",
        "\n",
        "    buffer_example_data = '['\n",
        "    for r in example_data:\n",
        "        buffer_example_data+='['\n",
        "        for c in r:\n",
        "            if(isinstance(c,str)):\n",
        "                buffer_example_data+=\"'\"+c+\"', \"\n",
        "            else:\n",
        "                buffer_example_data+=str(c)+\", \"\n",
        "        buffer_example_data= buffer_example_data[:-2]+\"], \\n\"\n",
        "    buffer_example_data= buffer_example_data[:-3]+\"]\"\n",
        "\n",
        "    report = \"\"\"\n",
        "Training Job Report    \n",
        "    \n",
        "Cross Validation Score: {cv_score}\n",
        "\n",
        "Training Model Parameters: {model_params}\n",
        "    \n",
        "Test Data Classification Report:\n",
        "{classification_report}\n",
        "\n",
        "Example of data array for prediciton:\n",
        "\n",
        "Order of columns:\n",
        "{columns}\n",
        "\n",
        "Example for clf.predict()\n",
        "{predict_example}\n",
        "\n",
        "\n",
        "Example of GCP API request body:\n",
        "{{\n",
        "    \"instances\": {json_example}\n",
        "}}\n",
        "\n",
        "\"\"\".format(\n",
        "        cv_score=cv_score,\n",
        "        model_params=json.dumps(model_params),\n",
        "        classification_report=classification_report,\n",
        "        columns = columns,\n",
        "        predict_example = buffer_example_data,\n",
        "        json_example = json.dumps(example_data.tolist()))\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def report_export_gcs(report: str, report_dir: str) -> None:\n",
        "    '''\n",
        "    Exports training job report to GCS\n",
        "\n",
        "            Parameters:\n",
        "                    report (str): Full report in text to sent to GCS\n",
        "                    report_dir (str): GCS path to store the report model. i.e gs://example_bucket/training-job\n",
        "            Returns:\n",
        "                    export_path (str): Report GCS location\n",
        "    '''\n",
        "    scheme, bucket, path, file = process_gcs_uri(report_dir)\n",
        "    if scheme != \"gs:\":\n",
        "        raise ValueError(\"URI scheme must be gs\")\n",
        "\n",
        "    # Upload the model to GCS\n",
        "    b = storage.Client().bucket(bucket)\n",
        "\n",
        "    export_path = os.path.join(path, 'report.txt')\n",
        "    blob = b.blob(export_path)\n",
        "\n",
        "    blob.upload_from_string(report)\n",
        "\n",
        "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
        "\n",
        "\n",
        "\n",
        "# Define all the command line arguments your model can accept for training\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Input Arguments\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--model_param_kernel',\n",
        "        help = 'SVC model parameter- kernel',\n",
        "        choices=['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
        "        type = str,\n",
        "        default = 'linear'\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--model_param_degree',\n",
        "        help = 'SVC model parameter- Degree. Only applies for poly kernel',\n",
        "        type = int,\n",
        "        default = 3\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--model_param_C',\n",
        "        help = 'SVC model parameter- C (regularization)',\n",
        "        type = float,\n",
        "        default = 1.0\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--model_param_probability',\n",
        "        help = 'Whether to enable probability estimates',\n",
        "        type = bool,\n",
        "        default = True\n",
        "    )\n",
        "\n",
        "\n",
        "    ''' \n",
        "    Vertex AI automatically populates a set of environment varialbes in the container that executes \n",
        "    your training job. those variables include:\n",
        "        * AIP_MODEL_DIR - Directory selected as model dir\n",
        "        * AIP_DATA_FORMAT - Type of dataset selected for training (can be csv or bigquery)\n",
        "    \n",
        "    Vertex AI will automatically split selected dataset into training,validation and testing\n",
        "    and 3 more environment variables will reflect the locaiton of the data:\n",
        "        * AIP_TRAINING_DATA_URI - URI of Training data\n",
        "        * AIP_VALIDATION_DATA_URI - URI of Validation data\n",
        "        * AIP_TEST_DATA_URI - URI of Test data\n",
        "        \n",
        "    Notice that those environment varialbes are default. If the user provides a value using CLI argument,\n",
        "    the environment variable will be ignored. If the user does not provide anything as CLI  argument\n",
        "    the program will try and use the environemnt variables if those exist. otherwise will leave empty.\n",
        "    '''\n",
        "    parser.add_argument(\n",
        "        '--model_dir',\n",
        "        help = 'Directory to output model and artifacts',\n",
        "        type = str,\n",
        "        default = os.environ['AIP_MODEL_DIR'] if 'AIP_MODEL_DIR' in os.environ else \"\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--data_format',\n",
        "        choices=['csv', 'bigquery'],\n",
        "        help = 'format of data uri csv for gs:// paths and bigquery for project.dataset.table formats',\n",
        "        type = str,\n",
        "        default =  os.environ['AIP_DATA_FORMAT'] if 'AIP_DATA_FORMAT' in os.environ else \"csv\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--training_data_uri',\n",
        "        help = 'location of training data in either gs:// uri or bigquery uri',\n",
        "        type = str,\n",
        "        default =  os.environ['AIP_TRAINING_DATA_URI'] if 'AIP_TRAINING_DATA_URI' in os.environ else \"\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--validation_data_uri',\n",
        "        help = 'location of validation data in either gs:// uri or bigquery uri',\n",
        "        type = str,\n",
        "        default =  os.environ['AIP_VALIDATION_DATA_URI'] if 'AIP_VALIDATION_DATA_URI' in os.environ else \"\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--test_data_uri',\n",
        "        help = 'location of test data in either gs:// uri or bigquery uri',\n",
        "        type = str,\n",
        "        default =  os.environ['AIP_TEST_DATA_URI'] if 'AIP_TEST_DATA_URI' in os.environ else \"\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\",\n",
        "                        action=\"store_true\")\n",
        "\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    arguments = args.__dict__\n",
        "\n",
        "\n",
        "    if args.verbose:\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "    logging.info('Model artifacts will be exported here: {}'.format(arguments['model_dir']))\n",
        "    logging.info('Data format: {}'.format(arguments[\"data_format\"]))\n",
        "    logging.info('Training data uri: {}'.format(arguments['training_data_uri']) )\n",
        "    logging.info('Validation data uri: {}'.format(arguments['validation_data_uri']))\n",
        "    logging.info('Test data uri: {}'.format(arguments['test_data_uri']))\n",
        "\n",
        "\n",
        "    '''\n",
        "    We have 2 different ways to load our data to pandas. One is from cloud storage by loading csv files and\n",
        "    the other is by connecting to BigQuery. Vertex AI supports both and \n",
        "    here we created a code that depelnding on the dataset provided, we will select the appropriated loading method.\n",
        "    '''\n",
        "    logging.info('Loading {} data'.format(arguments[\"data_format\"]))\n",
        "    if(arguments['data_format']=='csv'):\n",
        "        df_train = load_data_from_gcs(arguments['training_data_uri'])\n",
        "        df_test = load_data_from_gcs(arguments['test_data_uri'])\n",
        "        df_valid = load_data_from_gcs(arguments['validation_data_uri'])\n",
        "    elif(arguments['data_format']=='bigquery'):\n",
        "        print(arguments['training_data_uri'])\n",
        "        df_train = load_data_from_bq(arguments['training_data_uri'])\n",
        "        df_test = load_data_from_bq(arguments['test_data_uri'])\n",
        "        df_valid = load_data_from_bq(arguments['validation_data_uri'])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid data type \")\n",
        "\n",
        "    #as we will be using cross validation, we will have just a training set and a single test set.\n",
        "    # we ill merge the test and validation to achieve an 80%-20% split\n",
        "    df_test = pd.concat([df_test,df_valid])\n",
        "\n",
        "    logging.info('Defining model parameters')\n",
        "    model_params = dict()\n",
        "    model_params['kernel'] = arguments['model_param_kernel']\n",
        "    model_params['degree'] = arguments['model_param_degree']\n",
        "    model_params['C'] = arguments['model_param_C']\n",
        "    model_params['probability'] = arguments['model_param_probability']\n",
        "\n",
        "    df_train = clean_missing_numerics(df_train, NUMERIC_FEATURES)\n",
        "    df_test = clean_missing_numerics(df_test, NUMERIC_FEATURES)\n",
        "\n",
        "\n",
        "    logging.info('Running feature selection')\n",
        "    X_train, y_train = data_selection(df_train, ALL_COLUMNS, LABEL)\n",
        "    X_test, y_test = data_selection(df_test, ALL_COLUMNS, LABEL)\n",
        "\n",
        "    logging.info('Training pipelines in CV')\n",
        "    clf = pipeline_builder(model_params, BINARY_FEATURES_IDX, NUMERIC_FEATURES_IDX, CATEGORICAL_FEATURES_IDX)\n",
        "\n",
        "    cv_score = train_pipeline(clf, X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "    logging.info('Export trained pipeline and report')\n",
        "    pipeline_export_gcs(clf, arguments['model_dir'])\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "    test_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "\n",
        "    logging.info('f1score: '+ str(test_score))\n",
        "\n",
        "    report = prepare_report(cv_score,\n",
        "                            model_params,\n",
        "                            classification_report(y_test,y_pred),\n",
        "                            ALL_COLUMNS,\n",
        "                            X_test.to_numpy()[0:2])\n",
        "\n",
        "    report_export_gcs(report, arguments['model_dir'])\n",
        "\n",
        "\n",
        "    logging.info('Training job completed. Exiting...')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui7qPE3fmQem"
      },
      "source": [
        "### Install the training package locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUVXnoNhmQeo"
      },
      "outputs": [],
      "source": [
        "! cd titanic && python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_model"
      },
      "source": [
        "### 🏃 Train the model locally\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYtcuEG5mQeo"
      },
      "outputs": [],
      "source": [
        "CMDARGS = [f\"--model_param_kernel=linear\", \\\n",
        "           f\"--data_format=csv\", \\\n",
        "           f\"--training_data_uri=gs://{BUCKET_NAME}/data/titanic_train.csv\", \\\n",
        "           f\"--test_data_uri=gs://{BUCKET_NAME}/data/titanic_test.csv\", \\\n",
        "           f\"--validation_data_uri=gs://{BUCKET_NAME}/data/titanic_test.csv\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOMPSmXemQeo"
      },
      "outputs": [],
      "source": [
        "#add a specific path to write the model file to the args\n",
        "CMDARGS_LOCAL = \" \".join(CMDARGS + [f\"--model_dir=gs://{BUCKET_NAME}/titanic/trial\"])\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnb1EJdlmQep"
      },
      "outputs": [],
      "source": [
        "%run titanic/trainer/task.py $CMDARGS_LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpkHP6AdmQep"
      },
      "outputs": [],
      "source": [
        "# create a package and upload it to the cloud bucket\n",
        "! cd titanic && python setup.py sdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV9nL6dJmQep"
      },
      "outputs": [],
      "source": [
        "PACKAGE_URI = f\"gs://{BUCKET_NAME}/training/trainer-0.1.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9znT6i4XmQep",
        "outputId": "4fef3b79-674d-4ef3-c2d4-506fb5dc6cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://titanic/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][  6.3 KiB/  6.3 KiB]                                                \n",
            "Operation completed over 1 objects/6.3 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "! gsutil cp titanic/dist/trainer-0.1.tar.gz $PACKAGE_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job"
      },
      "source": [
        "## 🏃 Train and deploy the model on Vertex AI\n",
        "\n",
        "Define your custom `TrainingPipeline` on Vertex AI.\n",
        "\n",
        "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
        "\n",
        "- `display_name`: The user-defined name of this training pipeline.\n",
        "- `script_path`: The local path to the training script.\n",
        "- `container_uri`: The URI of the training container image.\n",
        "- `requirements`: The list of Python package dependencies of the script.\n",
        "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a pre-built container or a custom container.\n",
        "\n",
        "Use the `run` function to start training. The function takes the following parameters:\n",
        "\n",
        "- `args`: The command line arguments to be passed to the Python script.\n",
        "- `replica_count`: The number of worker replicas.\n",
        "- `model_display_name`: The display name of the `Model` if the script produces a managed `Model`.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "\n",
        "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxIxvDdglugx"
      },
      "outputs": [],
      "source": [
        "# Create a custom package-based training job\n",
        "JOB_NAME = 'superwise_vertex_demo_job'\n",
        "MODEL_DISPLAY_NAME = \"superwise_vertex_titanic\"\n",
        "\n",
        "\n",
        "job = aiplatform.CustomPythonPackageTrainingJob(display_name=JOB_NAME, \n",
        "                                                python_package_gcs_uri=PACKAGE_URI, \n",
        "                                                python_module_name='trainer.task', \n",
        "                                                container_uri=TRAIN_IMAGE, \n",
        "                                                model_serving_container_image_uri=DEPLOY_IMAGE, \n",
        "                                                )\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=MODEL_DISPLAY_NAME,\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        "    accelerator_count=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy_model:dedicated",
        "tags": []
      },
      "source": [
        "### 🚀 Deploy the model\n",
        "\n",
        "Before you use your model to make predictions, you must deploy it to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This will do two things:\n",
        "\n",
        "1. Create an `Endpoint` resource for deploying the `Model` resource to.\n",
        "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
        "\n",
        "\n",
        "The function takes the following parameters:\n",
        "\n",
        "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
        "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
        "   - If only one model, then specify `{ \"0\": 100 }`, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
        "   - If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify `{ \"0\": percent, model_id: percent, ... }`, where `model_id` is the ID of an existing `DeployedModel` on the endpoint. The percentages must add up to 100.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `starting_replica_count`: The number of compute instances to initially provision.\n",
        "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
        "\n",
        "### Traffic split\n",
        "\n",
        "The `traffic_split` parameter is specified as a Python dictionary. You can deploy more than one instance of your model to an endpoint, and then set the percentage of traffic that goes to each instance.\n",
        "\n",
        "You can use a traffic split to introduce a new model gradually into production. For example, if you had one existing model in production with 100% of the traffic, you could deploy a new model to the same endpoint, direct 10% of traffic to it, and reduce the original model's traffic to 90%. This allows you to monitor the new model's performance while minimizing the distruption to the majority of users.\n",
        "\n",
        "### Compute instance scaling\n",
        "\n",
        "You can specify a single instance (or node) to serve your online prediction requests. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
        "\n",
        "If you want to use multiple nodes to serve your online prediction requests, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n",
        "\n",
        "### Endpoint\n",
        "\n",
        "The method will block until the model is deployed and eventually return an `Endpoint` object. If this is the first time a model is deployed to the endpoint, it may take a few additional minutes to complete provisioning of resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMH7GrYMlugy"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "DEPLOYED_NAME = MODEL_DISPLAY_NAME + \"-\" + TIMESTAMP\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    deployed_model_display_name=DEPLOYED_NAME,\n",
        "    traffic_split=TRAFFIC_SPLIT,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_count=0,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_prediction",
        "tags": []
      },
      "source": [
        "## Make an online prediction request\n",
        "\n",
        "Send an online prediction request to your deployed model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_prediction_request:image"
      },
      "source": [
        "### Send the prediction request\n",
        "\n",
        "Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
        "\n",
        "- `instances`: A list of instances for prediction. Each instance is an array of values. \n",
        "\n",
        "**Note**: The first column for each instance needs to be the record_id. We are sending this to the prediction API in order to associate it with the prediction outputs on the server side.\n",
        "\n",
        "The `predict` function returns a list, where each element in the list corresponds to the an instance in the request. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e20473b09f5"
      },
      "outputs": [],
      "source": [
        "instances = train.to_numpy().tolist()\n",
        "predictions = endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ily0TwgmQer",
        "outputId": "e6d0915c-8c1f-4652-f35b-194a6e42e1fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_predicted = np.asarray(predictions.predictions, dtype= np.int)\n",
        "y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Ixcq19mQer",
        "outputId": "cf597514-49df-4a20-8a05-dce013f8c50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct predictions = 782, Total predictions = 916, Accuracy = 0.8537117903930131\n"
          ]
        }
      ],
      "source": [
        "correct = sum(y_predicted == np.array(y_train))\n",
        "accuracy = len(y_predicted)\n",
        "print(\n",
        "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz0nT9WemQes"
      },
      "source": [
        "## 📈 Part II - Setup Superwise.ai to track your model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnid-gB7mQes"
      },
      "source": [
        "### 🔧 Setup\n",
        "1. Install the Superwise Python package from pip\n",
        "2. Set environment variables with the API keys\n",
        "3. Create a Superwise client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qneZH39GmQes"
      },
      "outputs": [],
      "source": [
        "# # Login to Superwise.ai portal, and click on your account icon. \n",
        "# # Click \"personal tokens\" -> \"generate tokens\" and past the values below\n",
        "%env SUPERWISE_CLIENT_ID=REPLACE_WITH_YOUR_CLIENTID\n",
        "%env SUPERWISE_SECRET=REPLACE_WITH_YOUR_SECRET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72xsqJdimQes"
      },
      "outputs": [],
      "source": [
        "from superwise import Superwise\n",
        "from superwise.models.model import Model\n",
        "from superwise.models.version import Version\n",
        "from superwise.models.data_entity import DataEntity\n",
        "from superwise.resources.superwise_enums import FeatureType, DataEntityRole\n",
        "from superwise.controller.infer import infer_dtype\n",
        "\n",
        "sw = Superwise()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2cduPoEmQes"
      },
      "source": [
        "### Create a Superwise *Model*\n",
        "\n",
        "A *Model* represents a domain problem.\n",
        "In our case, the model is to predict the survival chances of the Titanic passengers.\n",
        "\n",
        "Over time, we may develop and deploy different ML models that attempt to address this model.\n",
        "\n",
        "In Superwise.ai terminology, each specific ML model we wish to track is called a *Version*. \n",
        "There may be multiple *Versions* belonging to a *Model* being tracked at any point in time (e.g. new models in shadow mode, or A/B tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR0F5B3PmQes",
        "outputId": "74bbf52b-29f5-4e40-8c8b-ad656fe32216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53\n"
          ]
        }
      ],
      "source": [
        "# Create the Model entity\n",
        "titanic_model =Model(\n",
        "    name=\"Superwise-vertex-titanic-model\",\n",
        "    description=\"Predicting Titanic passengers' survival probability\"\n",
        ")\n",
        "\n",
        "my_model = sw.model.create(titanic_model)\n",
        "print(my_model.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYnXjgmImQet"
      },
      "source": [
        "### Create a *Baseline* for our deployed model\n",
        "\n",
        "We've just deployed a model to Sagemaker, and wish to start tracking it.\n",
        "In order to perform the analysis of the model's performance over time, we need to set up a Baseline for the model's behavior.\n",
        "\n",
        "It's a common practice to use the training or test data (both features and predictions)as the baseline, as they represent \n",
        "The state which we consider stable and validated.\n",
        "\n",
        "Later, when the model performs predictions in production, we can compare the data and prediction behavior to the baseline, and detect drift.\n",
        "\n",
        "The baseline data includes:\n",
        "\n",
        "1. Features\n",
        "2. Labels\n",
        "3. Model predictions\n",
        "4. Timestamp of inference\n",
        "5. Id for each record (later used to correlate predictions with labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_wNPn7QmQet"
      },
      "outputs": [],
      "source": [
        "# add the prediction value, a timestamp and the label to the training features\n",
        "\n",
        "baseline_data = X_train.assign(\n",
        "    prediction=predictions.predictions,\n",
        "    ts=pd.Timestamp.now(),\n",
        "    survived=np.array(y_train)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbkK8wyomQet",
        "outputId": "3df06e6f-5e2c-4d06-d22d-f5d1ed23b2a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>record_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass</th>\n",
              "      <th>embarked</th>\n",
              "      <th>home_dest</th>\n",
              "      <th>parch</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>prediction</th>\n",
              "      <th>ts</th>\n",
              "      <th>survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1214</td>\n",
              "      <td>male</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>677</td>\n",
              "      <td>male</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>7.8958</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>Bulgaria Chicago, IL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>534</td>\n",
              "      <td>female</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>26.0000</td>\n",
              "      <td>2</td>\n",
              "      <td>S</td>\n",
              "      <td>Worcester, England</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1174</td>\n",
              "      <td>female</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>69.5500</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>864</td>\n",
              "      <td>female</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>7.7750</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>1095</td>\n",
              "      <td>female</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>7.6292</td>\n",
              "      <td>3</td>\n",
              "      <td>Q</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>912</th>\n",
              "      <td>1130</td>\n",
              "      <td>female</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>7.7750</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>1294</td>\n",
              "      <td>male</td>\n",
              "      <td>28.500000</td>\n",
              "      <td>16.1000</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>860</td>\n",
              "      <td>female</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>1126</td>\n",
              "      <td>female</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>7.8958</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 18:49:50.620685</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>916 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     record_id     sex        age     fare  pclass embarked  \\\n",
              "0         1214    male  29.881135   8.6625       3        S   \n",
              "1          677    male  26.000000   7.8958       3        S   \n",
              "2          534  female  19.000000  26.0000       2        S   \n",
              "3         1174  female  29.881135  69.5500       3        S   \n",
              "4          864  female  28.000000   7.7750       3        S   \n",
              "..         ...     ...        ...      ...     ...      ...   \n",
              "911       1095  female  29.881135   7.6292       3        Q   \n",
              "912       1130  female  18.000000   7.7750       3        S   \n",
              "913       1294    male  28.500000  16.1000       3        S   \n",
              "914        860  female  26.000000   7.9250       3        S   \n",
              "915       1126  female  28.000000   7.8958       3        S   \n",
              "\n",
              "                home_dest  parch  sibsp  prediction  \\\n",
              "0                       ?      0      0         0.0   \n",
              "1    Bulgaria Chicago, IL      0      0         0.0   \n",
              "2      Worcester, England      0      0         1.0   \n",
              "3                       ?      2      8         0.0   \n",
              "4                       ?      0      0         1.0   \n",
              "..                    ...    ...    ...         ...   \n",
              "911                     ?      0      0         1.0   \n",
              "912                     ?      0      0         1.0   \n",
              "913                     ?      0      0         0.0   \n",
              "914                     ?      0      0         1.0   \n",
              "915                     ?      0      0         1.0   \n",
              "\n",
              "                            ts  survived  \n",
              "0   2021-12-22 18:49:50.620685         0  \n",
              "1   2021-12-22 18:49:50.620685         0  \n",
              "2   2021-12-22 18:49:50.620685         1  \n",
              "3   2021-12-22 18:49:50.620685         0  \n",
              "4   2021-12-22 18:49:50.620685         0  \n",
              "..                         ...       ...  \n",
              "911 2021-12-22 18:49:50.620685         0  \n",
              "912 2021-12-22 18:49:50.620685         0  \n",
              "913 2021-12-22 18:49:50.620685         0  \n",
              "914 2021-12-22 18:49:50.620685         1  \n",
              "915 2021-12-22 18:49:50.620685         0  \n",
              "\n",
              "[916 rows x 12 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaVhh7g-mQet"
      },
      "source": [
        "### Create a *Schema* object that describes the format and sematics of our Baseline data\n",
        "\n",
        "The Schema object helps Superwise.ai interpret our data, for example - undertand which column prepresents predictions and which represents the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnwFHNLmQet"
      },
      "source": [
        "You can let Superwise to infer the data type, or you can verify and edit them manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeLm5BDimQet",
        "outputId": "60048ae3-a1b1-42ac-af1e-b96224deb4d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'record_id': 'Numeric',\n",
              " 'sex': 'Categorical',\n",
              " 'age': 'Numeric',\n",
              " 'fare': 'Numeric',\n",
              " 'pclass': 'Numeric',\n",
              " 'embarked': 'Categorical',\n",
              " 'home_dest': 'Categorical',\n",
              " 'parch': 'Numeric',\n",
              " 'sibsp': 'Numeric',\n",
              " 'prediction': 'Boolean',\n",
              " 'ts': 'Timestamp',\n",
              " 'survived': 'Boolean'}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entities_dtypes = infer_dtype(df=baseline_data)\n",
        "entities_dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9LNlPIYmQet"
      },
      "outputs": [],
      "source": [
        "entities_dtypes['pclass'] = 'Categorical'\n",
        "entities_dtypes['parch'] = 'Categorical'\n",
        "entities_dtypes['sibsp'] = 'Categorical'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSG6NLiTmQet"
      },
      "outputs": [],
      "source": [
        "entities_collection = sw.data_entity.summarise(\n",
        "    data=baseline_data,\n",
        "    entities_dtypes=entities_dtypes,\n",
        "    specific_roles = {\n",
        "      'record_id': DataEntityRole.ID,\n",
        "      'ts': DataEntityRole.TIMESTAMP,\n",
        "      'prediction': DataEntityRole.PREDICTION_VALUE,\n",
        "      'survived': DataEntityRole.LABEL\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x74X-DpKmQeu"
      },
      "source": [
        "Here are the schema main properties (roles, types, feature importance and descriptive statistics):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1tDPJo5mQeu",
        "outputId": "5e06c889-3317-43dd-ee32-de08d0f6bf32"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>type</th>\n",
              "      <th>role</th>\n",
              "      <th>feature_importance</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>record_id</td>\n",
              "      <td>Numeric</td>\n",
              "      <td>id</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'outlie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sex</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>74.11</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>age</td>\n",
              "      <td>Numeric</td>\n",
              "      <td>feature</td>\n",
              "      <td>5.14</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'outlie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fare</td>\n",
              "      <td>Numeric</td>\n",
              "      <td>feature</td>\n",
              "      <td>6.09</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'outlie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pclass</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>3.01</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>embarked</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>1.90</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>home_dest</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>parch</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>3.12</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>sibsp</td>\n",
              "      <td>Categorical</td>\n",
              "      <td>feature</td>\n",
              "      <td>6.64</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>prediction</td>\n",
              "      <td>Boolean</td>\n",
              "      <td>prediction value</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ts</td>\n",
              "      <td>Timestamp</td>\n",
              "      <td>time stamp</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0}}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>survived</td>\n",
              "      <td>Boolean</td>\n",
              "      <td>label</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{'statistics': {'missing_values': 0.0, 'new_va...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          name         type              role  feature_importance  \\\n",
              "0    record_id      Numeric                id                0.00   \n",
              "1          sex  Categorical           feature               74.11   \n",
              "2          age      Numeric           feature                5.14   \n",
              "3         fare      Numeric           feature                6.09   \n",
              "4       pclass  Categorical           feature                3.01   \n",
              "5     embarked  Categorical           feature                1.90   \n",
              "6    home_dest  Categorical           feature                0.00   \n",
              "7        parch  Categorical           feature                3.12   \n",
              "8        sibsp  Categorical           feature                6.64   \n",
              "9   prediction      Boolean  prediction value                0.00   \n",
              "10          ts    Timestamp        time stamp                0.00   \n",
              "11    survived      Boolean             label                0.00   \n",
              "\n",
              "                                              summary  \n",
              "0   {'statistics': {'missing_values': 0.0, 'outlie...  \n",
              "1   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "2   {'statistics': {'missing_values': 0.0, 'outlie...  \n",
              "3   {'statistics': {'missing_values': 0.0, 'outlie...  \n",
              "4   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "5   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "6             {'statistics': {'missing_values': 0.0}}  \n",
              "7   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "8   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "9   {'statistics': {'missing_values': 0.0, 'new_va...  \n",
              "10            {'statistics': {'missing_values': 0.0}}  \n",
              "11  {'statistics': {'missing_values': 0.0, 'new_va...  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ls = list()\n",
        "for entity in entities_collection:\n",
        "    ls.append(entity.get_properties())\n",
        "    \n",
        "pd.DataFrame(ls)[['name', 'type', 'role', 'feature_importance', 'summary']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wCs8Rn2mQeu"
      },
      "source": [
        "### Create a *Version* object\n",
        "\n",
        "As explained above, a *Version* represents a concrete ML model we are tracking.\n",
        "\n",
        "A *Version* solves a *Model*\n",
        "\n",
        "A *Version* has a *Baseline*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYPJ7g0AmQeu"
      },
      "outputs": [],
      "source": [
        "titanic_version = Version(\n",
        "    model_id=my_model.id,\n",
        "    name=\"1.0\",\n",
        "    data_entities=entities_collection,\n",
        ")\n",
        "\n",
        "my_version = sw.version.create(titanic_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh5lUs0ZmQeu",
        "outputId": "a0539eea-e86a-4f65-fc17-f83782609a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Response [204]>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sw.version.activate(my_version.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Q3bICkmQeu"
      },
      "source": [
        "## 🩺 Part III - monitoring ongoing predictions\n",
        "\n",
        "Now that we have a *Version* of the model setup with a *Baseline*, we can start sending ongoing model predictions to Superwise to monitor the model's performance in a production settings.\n",
        "\n",
        "For this demo, we will treat the Test split of the data as our \"ongoing predictions\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLTolXXSmQeu"
      },
      "outputs": [],
      "source": [
        "predictions = endpoint.predict(instances=X_test.to_numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_qzwnn0mQeu",
        "outputId": "69ec43e8-e1ed-4ce9-f333-3d02304b65ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>record_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass</th>\n",
              "      <th>embarked</th>\n",
              "      <th>home_dest</th>\n",
              "      <th>parch</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>prediction</th>\n",
              "      <th>ts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1148</td>\n",
              "      <td>male</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>7.1250</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1049</td>\n",
              "      <td>male</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>15.7417</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>982</td>\n",
              "      <td>male</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>7.8958</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>808</td>\n",
              "      <td>male</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>Bridgwater, Somerset, England</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1195</td>\n",
              "      <td>male</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>3</td>\n",
              "      <td>Q</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>325</td>\n",
              "      <td>male</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>2</td>\n",
              "      <td>S</td>\n",
              "      <td>Bryn Mawr, PA, USA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>919</td>\n",
              "      <td>male</td>\n",
              "      <td>18.500000</td>\n",
              "      <td>7.2292</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>532</td>\n",
              "      <td>male</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>2</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>1159</td>\n",
              "      <td>female</td>\n",
              "      <td>29.881135</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>513</td>\n",
              "      <td>female</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>30.0708</td>\n",
              "      <td>2</td>\n",
              "      <td>C</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2021-12-22 19:03:22.415855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>786 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     record_id     sex        age     fare  pclass embarked  \\\n",
              "0         1148    male  35.000000   7.1250       3        S   \n",
              "1         1049    male  20.000000  15.7417       3        C   \n",
              "2          982    male  29.881135   7.8958       3        S   \n",
              "3          808    male  29.881135   8.0500       3        S   \n",
              "4         1195    male  29.881135   7.7500       3        Q   \n",
              "..         ...     ...        ...      ...     ...      ...   \n",
              "388        325    male  30.000000  13.0000       2        S   \n",
              "389        919    male  18.500000   7.2292       3        C   \n",
              "390        532    male  41.000000  13.0000       2        S   \n",
              "391       1159  female  29.881135   8.0500       3        S   \n",
              "392        513  female  14.000000  30.0708       2        C   \n",
              "\n",
              "                         home_dest  parch  sibsp  prediction  \\\n",
              "0                                ?      0      0         0.0   \n",
              "1                                ?      1      1         0.0   \n",
              "2                                ?      0      0         0.0   \n",
              "3    Bridgwater, Somerset, England      0      0         0.0   \n",
              "4                                ?      0      0         0.0   \n",
              "..                             ...    ...    ...         ...   \n",
              "388             Bryn Mawr, PA, USA      0      0         0.0   \n",
              "389                              ?      0      0         0.0   \n",
              "390                              ?      0      0         0.0   \n",
              "391                              ?      0      0         1.0   \n",
              "392                   New York, NY      0      1         1.0   \n",
              "\n",
              "                            ts  \n",
              "0   2021-12-22 19:03:22.415855  \n",
              "1   2021-12-22 19:03:22.415855  \n",
              "2   2021-12-22 19:03:22.415855  \n",
              "3   2021-12-22 19:03:22.415855  \n",
              "4   2021-12-22 19:03:22.415855  \n",
              "..                         ...  \n",
              "388 2021-12-22 19:03:22.415855  \n",
              "389 2021-12-22 19:03:22.415855  \n",
              "390 2021-12-22 19:03:22.415855  \n",
              "391 2021-12-22 19:03:22.415855  \n",
              "392 2021-12-22 19:03:22.415855  \n",
              "\n",
              "[786 rows x 11 columns]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = [x for x in predictions.predictions]\n",
        "ongoing_predictions = X_test.copy()\n",
        "ongoing_predictions['prediction']=pred\n",
        "ongoing_predictions['ts']=pd.Timestamp.now()\n",
        "ongoing_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNXD5Mj2mQev",
        "outputId": "4d7e29c4-34de-4461-8474-ad7c2259c72d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'transaction_id': '0a50ac7a-635a-11ec-99d4-5acaded3d43d'}\n"
          ]
        }
      ],
      "source": [
        "transaction_id = sw.transaction.log_records(\n",
        "    model_id=my_model.id,\n",
        "    version_id=my_version.name,\n",
        "    records=ongoing_predictions.to_dict(orient='records')\n",
        ")\n",
        "print(transaction_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3J26HRdmQev",
        "outputId": "439ff598-63a4-4dfb-9652-ecf50b00daea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Passed'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transaction_id = sw.transaction.get(transaction_id=transaction_id['transaction_id'])\n",
        "transaction_id.get_properties()['status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsA0_GLumQev"
      },
      "source": [
        "### Optional - report ongoing lables to Superwise.ai\n",
        "\n",
        "In some cases, our system is able to gather \"ground truth\" labels for it's predictions.\n",
        "Often, this happens later on, after the prediciton was already given.\n",
        "\n",
        "By sending these labels to Superwise.ai, we add another important layer of data to our monitoring solution.\n",
        "\n",
        "For the purpose of this demo, we can use the test set's labels as the ground truth, simulating a label we collected in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NGFD-GnumQev",
        "outputId": "fddfdad7-99d9-4de9-a430-86129dda2e3c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>record_id</th>\n",
              "      <th>survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>1148</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>1049</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>982</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>808</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1195</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>919</th>\n",
              "      <td>919</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>532</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>1159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>513</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>393 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      record_id  survived\n",
              "1148       1148         0\n",
              "1049       1049         1\n",
              "982         982         0\n",
              "808         808         0\n",
              "1195       1195         0\n",
              "...         ...       ...\n",
              "325         325         0\n",
              "919         919         0\n",
              "532         532         0\n",
              "1159       1159         1\n",
              "513         513         1\n",
              "\n",
              "[393 rows x 2 columns]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Note: we provide the column names we declared in the Schema object, \n",
        "# so that Superwise.ai will be able to interpret the data\n",
        "\n",
        "ground_truth = pd.DataFrame(data=test, columns=['record_id', 'survived'])\n",
        "ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8icqmFc6mQev",
        "outputId": "4892814f-0b69-4529-b94d-68606aabe23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'transaction_id': '651bee30-635a-11ec-ad4d-426f0aedd514'}\n"
          ]
        }
      ],
      "source": [
        "transaction_id = sw.transaction.log_records(\n",
        "    model_id=my_model.id,\n",
        "    records=ground_truth.to_dict(orient='records')\n",
        ")\n",
        "print(transaction_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkYDRU00mQev",
        "outputId": "28a352f4-c75b-4344-83ab-72c460b65e35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Passed'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transaction_id = sw.transaction.get(transaction_id=transaction_id['transaction_id'])\n",
        "transaction_id.get_properties()['status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "undeploy_model"
      },
      "source": [
        "## Undeploy the model\n",
        "\n",
        "To undeploy your `Model` resource from the serving `Endpoint` resource, use the endpoint's `undeploy` method with the following parameter:\n",
        "\n",
        "- `deployed_model_id`: The model deployment identifier returned by the endpoint service when the `Model` resource was deployed. You can retrieve the deployed models using the endpoint's `deployed_models` property.\n",
        "\n",
        "Since this is the only deployed model on the `Endpoint` resource, you can omit `traffic_split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPSAO1tlug0"
      },
      "outputs": [],
      "source": [
        "deployed_model_id = endpoint.list_models()[0].id\n",
        "endpoint.undeploy(deployed_model_id=deployed_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:custom"
      },
      "source": [
        "## 🗑️ Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Training Job\n",
        "- Model\n",
        "- Endpoint\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNmebHf7lug0"
      },
      "outputs": [],
      "source": [
        "delete_training_job = True\n",
        "delete_model = True\n",
        "delete_endpoint = True\n",
        "\n",
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "# Delete the training job\n",
        "job.delete()\n",
        "\n",
        "# Delete the model\n",
        "model.delete()\n",
        "\n",
        "# Delete the endpoint\n",
        "endpoint.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vertex.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "common-cpu.m80",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
